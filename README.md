# ğŸ¤– Oh my AI!

A modern web application that runs a Large Language Model (LLM) entirely in your browser using WebAssembly.

## âœ¨ Features

- ğŸš€ **Local execution**: The model runs 100% in your browser via WebAssembly
- ğŸ”’ **Complete privacy**: No data is sent to external servers
- ğŸ’¬ **Modern chat interface**: Elegant UI built with TailwindCSS
- âš¡ **Real-time streaming**: Responses appear as they are generated
- ğŸ¨ **Responsive design**: Works perfectly on desktop and mobile

## ğŸ› ï¸ Technologies

- **SvelteKit**: Modern and performant web framework
- **Bun**: Ultra-fast JavaScript runtime
- **TailwindCSS**: Utility-first CSS framework
- **WebLLM**: Library to run LLMs in WASM
- **WebAssembly**: Compilation for native performance

## ğŸ“‹ Prerequisites

- **Bun**: Version 1.0 or higher
- **Modern browser**: Chrome, Firefox, Safari or Edge with WebAssembly support
- **Memory**: At least 4 GB RAM recommended

## ğŸš€ Installation

### 1. Clone the project

```sh
git clone https://github.com/BonoAI-org/ohmyai.git
cd ohmyai
```

### 2. Install dependencies

```sh
bun install
```

### 3. Start development server

```sh
bun run dev
```

The application will be available at `http://localhost:5173`

### 4. Open in browser

Open your browser and go to the displayed URL. The model will start downloading automatically.

## ğŸ“¦ Build for production

To create an optimized production version:

```sh
bun run build
```

To preview the production build:

```sh
bun run preview
```

## ğŸ¯ Usage

### First use

1. **Model loading**: On first load, the model (~1-2 GB) will be downloaded and cached
2. **Wait for loading**: A progress bar will show the download status
3. **Start chatting**: Once loaded, type your message and press Enter or click the send button

### Interface features

- **Enter**: Send a message
- **Shift + Enter**: New line
- **ğŸ—‘ï¸ Button**: Clear conversation

## ğŸ§© Project architecture

```
ohmyai/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ lib/
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â””â”€â”€ ChatMessage.svelte     # Message component
â”‚   â”‚   â””â”€â”€ stores/
â”‚   â”‚       â””â”€â”€ llm.svelte.js          # LLM management store
â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”œâ”€â”€ +layout.svelte             # Main layout
â”‚   â”‚   â””â”€â”€ +page.svelte               # Chat page
â”‚   â”œâ”€â”€ app.css                        # Global styles
â”‚   â””â”€â”€ app.html                       # HTML template
â”œâ”€â”€ static/                            # Static files
â”œâ”€â”€ package.json                       # Dependencies
â”œâ”€â”€ svelte.config.js                   # Svelte configuration
â”œâ”€â”€ vite.config.js                     # Vite configuration
â””â”€â”€ tailwind.config.js                 # Tailwind configuration
```

## ğŸ”§ Configuration

### Change the model

To use a different model, modify the `selectedModel` value in `/src/lib/stores/llm.svelte.js`:

```javascript
selectedModel = $state('Llama-3.2-1B-Instruct-q4f32_1-MLC');
```

Available models:
- `Llama-3.2-1B-Instruct-q4f32_1-MLC` (lightweight)
- `Llama-3.2-3B-Instruct-q4f32_1-MLC` (balanced)
- `Phi-3.5-mini-instruct-q4f16_1-MLC` (fast)
- See full list at: https://github.com/mlc-ai/web-llm

### Adjust generation parameters

In `/src/lib/stores/llm.svelte.js`, `sendMessage()` method:

```javascript
const asyncChunkGenerator = await this.engine.chat.completions.create({
	messages: chatMessages,
	temperature: 0.7,      // Creativity (0-1)
	max_tokens: 512,       // Max length
	stream: true,
});
```

## ğŸ› Troubleshooting

### Model won't load

- Check your internet connection
- Make sure you have enough disk space (browser cache)
- Try a smaller model

### Memory error

- Close other tabs
- Use a smaller model
- Increase available RAM for your browser

### Slow performance

- Use a Chromium-based browser (Chrome, Edge) for better performance
- Make sure hardware acceleration is enabled in browser settings
- Try a smaller quantized model

## ğŸ“š Documentation

### Complete Guides

- ğŸš€ **[Quick Start](docs/QUICKSTART.md)** - Quick start guide
- ğŸ—ï¸ **[Architecture](docs/ARCHITECTURE.md)** - Project architecture
- ğŸŒ **[Deployment](docs/DEPLOYMENT.md)** - Complete deployment guide
- âš¡ **[Cloudflare Quick Deploy](docs/CLOUDFLARE_QUICKSTART.md)** - Cloudflare deployment
- ğŸ“± **[PWA Guide](docs/PWA_GUIDE.md)** - Progressive Web App features
- ğŸ’¾ **[RAM Check](docs/RAM_CHECK.md)** - RAM detection

### Features

- ğŸ¤– **[Models](docs/MODELES.md)** - Available models list
- â• **[Custom Models](docs/CUSTOM_MODELS.md)** - Add custom models
- ğŸ’¬ **[History](docs/CONVERSATION_HISTORY.md)** - Conversation management
- ğŸ—„ï¸ **[Dexie Migration](docs/DEXIE_MIGRATION.md)** - Database migration
- ğŸ—ºï¸ **[Sitemap](docs/SITEMAP.md)** - SEO sitemap configuration

### Development

- ğŸ¤ **[Contributing](docs/CONTRIBUTING.md)** - Contribution guide
- ğŸ“Š **[Project Summary](docs/PROJECT_SUMMARY.md)** - Project summary

## ğŸ“š External Resources

- [WebLLM Documentation](https://github.com/mlc-ai/web-llm)
- [SvelteKit Documentation](https://kit.svelte.dev/)
- [Bun Documentation](https://bun.sh/)
- [TailwindCSS](https://tailwindcss.com/)

## ğŸŒ About

This project is developed by [BonoAI](https://bonoai.org) - Building the future of local AI applications.

**Repository**: [github.com/BonoAI-org/ohmyai](https://github.com/BonoAI-org/ohmyai)

Visit [bonoai.org](https://bonoai.org) to discover more innovative AI projects.

## ğŸ“ License

MIT

## ğŸ¤ Contributing

Contributions are welcome! Feel free to open an issue or pull request.
